import pandas as pd
import random
from keras_sequence import KerasSequence
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.text import one_hot 

import numpy as np
import tqdm
import tensorflow as tf
import tensorflow.keras as keras
import pickle as pkl


def read_csv(path:str):
    df = pd.read_csv(path)
    train = df.values[:85804]
    test = df.values[85804:]
    return train, test


class Entree():
    def __init__(self, code:str, line_id, labels):
        self.code = code
        self.line_id = line_id
        self.labels = labels
        self.tokenized = None
    
    def x(self):
        return self.tokenized
    
    def y(self):
        return np.array(self.labels)

def add_spaces(code:str, seq:str):
    return code.replace(seq, ' ' + seq + ' ')

def code_preprocessing(code):
    for c in '{};*=[]();,.<>':
        code = add_spaces(code, c)
    code = add_spaces(code, '->')
    code = add_spaces(code, '//')
    code = add_spaces(code, '||')
    code = add_spaces(code, '&&')
    code = code.replace('_', ' ')
    code = code.replace(' float ', ' |TYPE| ')
    code = code.replace(' int ', ' |TYPE| ')
    code = code.replace(' double ', ' |TYPE| ')
    code = code.replace(' char ', ' |TYPE| ')
    code = code.replace(' string ', ' |TYPE| ')
    code = code.replace(' bool ', ' |TYPE| ')

    code = code.split(' ')
    code = [c.strip() for c in code if len(c.strip())]
    code = ' '.join(code)
    return code

def create_test_dataset(test:np.array):
    entrees = []
    for line in test:
        line_id = line[0]
        code = code_preprocessing(line[1])
        entrees.append(Entree(code, line_id, 'patate'))
    return entrees

def create_train_dataset(train:np.array):
    train_entrees = []
    for line in train:
        line_id = line[0]
        code = code_preprocessing(line[1])
        labels = line[2:]
        train_entrees.append(Entree(code, line_id, labels))
    return train_entrees

def create_dataset_and_tokenizer(train:np.array, test:np.array):
    test_entrees = create_test_dataset(test)
    train_entrees = create_train_dataset(train)

    tokenizer = Tokenizer()
    tokenizer.fit_on_texts((e.code for e in tqdm.tqdm(train_entrees+test_entrees)))
    X = tokenizer.texts_to_sequences((e.code for e in tqdm.tqdm(train_entrees+test_entrees)))
    X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=400)
    X_train = X[:len(train_entrees)]
    X_test = X[len(train_entrees):]

    y_train = np.array([e.y() for e in train_entrees])
    
    with open('dataset.pkl', 'wb') as f:
        pkl.dump(X_train, f)
        pkl.dump(X_test, f)
        pkl.dump(y_train, f)
    
    ids_test = [e.line_id for e in test_entrees]
    
    
    with open('dataset.pkl', 'rb') as f:
        X_train = pkl.load(f)
        X_test = pkl.load(f)
        y_train = pkl.load(f)
    
    return X_train, X_test, y_train, ids_test
